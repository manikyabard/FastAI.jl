<HTML><head><title>Tabular Classification</title><script src=../template/highlight.min.js ></script><script src=../template/julia.min.js ></script><script src=../template/loadhighlightjs.js ></script><link href=../template/ansi.css rel=stylesheet ></link><link href=../template/hugobook.css rel=stylesheet ></link><meta content=Type=text/html; charset=utf-8 http-equiv=Content-Type ></meta><meta name=viewport content=width=device-width, initial-scale=1 ></meta></head><body><input onclick=toggleMenu() id=menu-control class=hidden toggle type=checkbox ></input><input id=toc-control type=checkbox class=hidden toggle ></input><main class=container flex ><aside id=menu-container class=book-menu ><nav class=book-menu-content ><h2 id=title >FastAI.jl</h2><div id=sidebar ><div class=doctree ><body><ul><li><p><a href=../README.md.html title= >README</a></p></li><li><p><a href=../docs/setup.md.html title= >Setup</a></p></li><li><p><a href=quickstart.ipynb.html title= >Quickstart</a></p></li><li><p>Tutorials</p><ul><li><p>Beginner</p><ul><li><p><a href=../docs/introduction.md.html title= >Introduction</a></p></li><li><p><a href=../docs/discovery.md.html title= >Discovery</a></p></li></ul></li><li><p>Intermediate</p><ul><li><p><a href=imagesegmentation.ipynb.html title= >Image segmentation</a></p></li><li><p><a href=keypointregression.ipynb.html title= >Keypoint regression</a></p></li><li><p><a href=tabularclassification.ipynb.html title= >Tabular classification</a></p></li><li><p><a href=../docs/data_containers.md.html title= >Data containers</a></p></li><li><p><a href=serialization.ipynb.html title= >Saving and loading models</a></p></li></ul></li><li><p>Advanced</p><ul><li><p><a href=presizing.ipynb.html title= >Presizing vision datasets</a></p></li><li><p><a href=../docs/learning_methods.md.html title= >Custom Learning methods</a></p></li></ul></li></ul></li><li><p>How To</p><ul><li><p><a href=training.ipynb.html title= >Train your model</a></p></li><li><p><a href=../docs/howto/augmentvision.md.html title= >Augment vision data</a></p></li><li><p><a href=../docs/howto/logtensorboard.md.html title= >Log to TensorBoard</a></p></li></ul></li><li><p>Reference</p><ul><li><p><a href=../REFERENCE.html title= >Docstrings</a></p></li><li><p><a href=../docs/fastai_api_comparison.md.html title= >fastai API comparison</a></p></li><li><p><a href=../docs/interfaces.md.html title= >Extension APIs</a></p></li><li><p><a href=../docs/glossary.md.html title= >Glossary</a></p></li></ul></li><li><p>Background</p><ul><li><p><a href=../docs/background/blocksencodings.md.html title= >Blocks and encodings</a></p></li><li><p><a href=../docs/background/datapipelines.md.html title= >Performant data pipelines</a></p></li></ul></li></ul></body></div></div></nav></aside><div class=book-page ><header class=book-header ></header><article><h1 id=tabular-classification >Tabular Classification</h1><p>Tabular Classification involves having a categorical column as the target. Here, we’ll use the adult sample dataset from fastai and try to predict whether the salary is above 50K or not, making this a binary classification task.</p><pre lang=julia ><code>using Flux
using FastAI
using Tables
using Statistics
using FluxTraining
import DataAugmentation</code></pre><p>We can quickly download and get the path of any dataset from fastai by using <a href=../REFERENCE/FastAI.Datasets.Datasets.datasetpath.html ><code>datasetpath</code></a>. Once we have the path, we’ll load the data in a <a href=../REFERENCE/FastAI.TableDataset.html ><code>TableDataset</code></a>. By default, if we pass in just the path to <a href=../REFERENCE/FastAI.TableDataset.html ><code>TableDataset</code></a>, the data is loaded in a <code>DataFrame</code>, but we can use any package for accessing our data, and pass an object satisfying the <a href=https://github.com/JuliaData/Tables.jl title= >Tables.jl</a> interface to it.</p><pre lang=julia ><code>data = TableDataset(joinpath(datasetpath(&quot;adult_sample&quot;), &quot;adult.csv&quot;))</code></pre><pre class=coderesult ><code>TableDataset{DataFrames.DataFrame}([1m32561×15 DataFrame[0m
[1m   Row [0m│[1m age   [0m[1m workclass         [0m[1m fnlwgt [0m[1m education     [0m[1m education-num [0m[1m marit[0m ⋯
[1m       [0m│[90m Int64 [0m[90m String            [0m[90m Int64  [0m[90m String        [0m[90m Float64?      [0m[90m Strin[0m ⋯
───────┼────────────────────────────────────────────────────────────────────────
     1 │    49   Private           101320   Assoc-acdm             12.0   Marr ⋯
     2 │    44   Private           236746   Masters                14.0   Divo
     3 │    38   Private            96185   HS-grad      [90m     missing   [0m  Divo
     4 │    38   Self-emp-inc      112847   Prof-school            15.0   Marr
     5 │    42   Self-emp-not-inc   82297   7th-8th      [90m     missing   [0m  Marr ⋯
     6 │    20   Private            63210   HS-grad                 9.0   Neve
     7 │    49   Private            44434   Some-college           10.0   Divo
     8 │    37   Private           138940   11th                    7.0   Marr
     9 │    46   Private           328216   HS-grad                 9.0   Marr ⋯
    10 │    36   Self-emp-inc      216711   HS-grad      [90m     missing   [0m  Marr
    11 │    23   Private           529223   Bachelors              13.0   Neve
   ⋮   │   ⋮            ⋮            ⋮           ⋮              ⋮              ⋱
 32552 │    60   Private           230545   7th-8th                 4.0   Divo
 32553 │    39   Private           139743   HS-grad                 9.0   Sepa ⋯
 32554 │    35   Self-emp-inc      135436   Prof-school            15.0   Marr
 32555 │    53   Private            35102   Some-college           10.0   Divo
 32556 │    48   Private           355320   Bachelors              13.0   Marr
 32557 │    36   Private           297449   Bachelors              13.0   Divo ⋯
 32558 │    23   ?                 123983   Bachelors              13.0   Neve
 32559 │    53   Private           157069   Assoc-acdm             12.0   Marr
 32560 │    32   Local-gov         217296   HS-grad                 9.0   Marr
 32561 │    26   Private           182308   Some-college           10.0   Marr ⋯
[36m                                               10 columns and 32540 rows omitted[0m)</code></pre><p>In case our data was present in a different format for eg. parquet, it could be loaded into a data container as follows:</p><pre lang=julia ><code>
using Parquet

TableDataset(read_parquet(parquet_path));

</code></pre><p><a href=../REFERENCE/FastAI.Datasets.Datasets.mapobs.html ><code>mapobs</code></a> is used here to split our target column from the rest of the row in a lazy manner, so that each observation consists of a row of inputs and a target variable.</p><pre lang=julia ><code>splitdata = mapobs(row -&gt; (row, row[:salary]), data);</code></pre><p>To create a learning method for tabular classification task, we need an input block, an output block, and the encodings to be performed on the data.</p><p>The input block here is a <a href=../REFERENCE/FastAI.TableRow.html ><code>TableRow</code></a> which contains information about the nature of the columns (ie. categorical or continuous) along with an indexable collection mapping categorical column names to a collection with distinct classes in that column. We can get this mapping by using the <code>gettransformationdict</code> method with <a href=../REFERENCE/DataAugmentation.Categorify.html ><code>DataAugmentation.Categorify</code></a>.</p><p>The outblock block used is <a href=../REFERENCE/FastAI.Label.html ><code>Label</code></a> for single column classification and the unique classes have to passed to it.</p><p>This is followed by the encodings which needs to be applied on our input and output blocks. For the input block, we have used the <code>gettransforms</code> function here to get a standard bunch of transformations to apply, but this can be easily customized by passing in any tabular transformation from DataAugmentation.jl or a composition of those, to <a href=../REFERENCE/FastAI.TabularPreprocessing.html ><code>TabularPreprocessing</code></a>. In addition to this, we have just one-hot encoded the outblock.</p><pre lang=julia ><code>cat, cont = FastAI.getcoltypes(data)
target = :salary
cat = filter(!isequal(target), cat)
catdict = FastAI.gettransformdict(data, DataAugmentation.Categorify, cat);</code></pre><pre lang=julia ><code>inputblock = TableRow(cat, cont, catdict)
targetblock = Label(unique(data.table[:, target]))

method = BlockMethod(
    (inputblock, targetblock),
    (
        setup(TabularPreprocessing, inputblock, data),
        FastAI.OneHot()
    )
)</code></pre><pre class=codeoutput ><code>┌ Warning: There is a missing value present for category &#39;occupation&#39; which will be removed from Categorify dict
└ @ DataAugmentation /home/lorenz/.julia/dev/DataAugmentation/src/rowtransforms.jl:108
</code></pre><pre class=coderesult ><code>BlockMethod(TableRow{8, 6, Dict{Any, Any}} -> Label{String})</code></pre><p>In case our initial problem wasn’t a classification task, and we had a continuous target column, we would need to perform tabular regression. To create a learning method suitable for regression, we use a <a href=../REFERENCE/FastAI.Continuous.html ><code>Continuous</code></a> block for representing our target column. This can be done even with multiple continuous target columns by just passing the number of columns in <code>Continuous</code>. For example, the method here could be used for 3 targets.</p><pre lang=julia ><code>
method2 = BlockMethod(

    (

        TableRow(cat, cont, catdict), 

        Continuous(3)

    ),

    ((FastAI.TabularPreprocessing(data),)),

    outputblock = Continuous(3)

)

</code></pre><p>To get an overview of the learning method created, and as a sanity test, we can use <a href=../REFERENCE/FastAI.describemethod.html ><code>describemethod</code></a>. This shows us what encodings will be applied to which blocks, and how the predicted ŷ values are decoded.</p><pre lang=julia ><code>describemethod(method)</code></pre><pre class=coderesult ><code>[1m  [36mLearningMethod[39m summary[22m
[1m  ------------------------[22m

    •  Task: [36mTableRow{8, 6, Dict{Any, Any}} -> Label{String}[39m

    •  Model blocks: [36mFastAI.EncodedTableRow{8, 6, Dict{Any, Any}} ->
       FastAI.OneHotTensor{0, String}[39m

  Encoding a sample ([36mencode(method, context, sample)[39m)

              Encoding            Name                             [36mmethod.blocks[1][39m               [36mmethod.blocks[2][39m
  –––––––––––––––––––– ––––––––––––––– –––––––––––––––––––––––––––––––––––––––––––– ––––––––––––––––––––––––––––––
                       [36m(input, target)[39m               [36mTableRow{8, 6, Dict{Any, Any}}[39m                  [36mLabel{String}[39m
  [36mTabularPreprocessing[39m                 [1m[36mFastAI.EncodedTableRow{8, 6, Dict{Any, Any}}[39m[22m                  [36mLabel{String}[39m
                [36mOneHot[39m          [36m(x, y)[39m [36mFastAI.EncodedTableRow{8, 6, Dict{Any, Any}}[39m [1m[36mFastAI.OneHotTensor{0, String}[39m[22m

  Decoding a model output ([36mdecode(method, context, ŷ)[39m)

              Decoding        Name             [36mmethod.outputblock[39m
  –––––––––––––––––––– ––––––––––– ––––––––––––––––––––––––––––––
                                [36mŷ[39m [36mFastAI.OneHotTensor{0, String}[39m
                [36mOneHot[39m                              [1m[36mLabel{String}[39m[22m
  [36mTabularPreprocessing[39m [36mtarget_pred[39m                  [36mLabel{String}[39m</code></pre><p><code>getobs</code> gets us a row of data from the <code>TableDataset</code>, which we encode here. This gives us a tuple with the input and target. The input here is again a tuple, containing the categorical values (which have been label encoded or “categorified”) and the continuous values (which have been normalized and any missing values have been filled).</p><pre lang=julia ><code>getobs(splitdata, 1000)</code></pre><pre class=coderesult ><code>([1mDataFrameRow[0m
[1m  Row [0m│[1m age   [0m[1m workclass  [0m[1m fnlwgt [0m[1m education [0m[1m education-num [0m[1m marital-status   [0m ⋯
[1m      [0m│[90m Int64 [0m[90m String     [0m[90m Int64  [0m[90m String    [0m[90m Float64?      [0m[90m String           [0m ⋯
──────┼─────────────────────────────────────────────────────────────────────────
 1000 │    61   State-gov  162678   5th-6th             3.0   Married-civ-spou ⋯
[36m                                                              10 columns omitted[0m, "<50k")</code></pre><pre lang=julia ><code>x = encode(method, Training(), getobs(splitdata, 1000))</code></pre><pre class=coderesult ><code>(([5, 16, 2, 10, 5, 2, 3, 2], [1.6435221651965317, -0.2567538819371021, -2.751580937680526, -0.14591824281680102, -0.21665620002803673, -0.035428902921319616]), Float32[0.0, 1.0])</code></pre><p>To get a model suitable for our learning method, we can use <a href=../REFERENCE/DLPipelines.methodmodel.html ><code>DLPipelines.methodmodel</code></a> which constructs a suitable model based on the target block.</p><pre lang=julia ><code>model = methodmodel(method)</code></pre><pre class=coderesult ><code>Chain(
  Parallel(
    vcat,
    Chain(
      FastAI.Models.var"#41#43"(),
      Parallel(
        vcat,
        Embedding(10, 6),               [90m# 60 parameters[39m
        Embedding(17, 8),               [90m# 136 parameters[39m
        Embedding(8, 5),                [90m# 40 parameters[39m
        Embedding(17, 8),               [90m# 136 parameters[39m
        Embedding(7, 5),                [90m# 35 parameters[39m
        Embedding(6, 4),                [90m# 24 parameters[39m
        Embedding(3, 3),                [90m# 9 parameters[39m
        Embedding(43, 13),              [90m# 559 parameters[39m
      ),
      identity,
    ),
    BatchNorm(6),                       [90m# 12 parameters[39m[90m, plus 12[39m
  ),
  Chain(
    Dense(58, 200, relu; bias=false),   [90m# 11_600 parameters[39m
    BatchNorm(200),                     [90m# 400 parameters[39m[90m, plus 400[39m
    identity,
  ),
  Chain(
    Dense(200, 100, relu; bias=false),  [90m# 20_000 parameters[39m
    BatchNorm(100),                     [90m# 200 parameters[39m[90m, plus 200[39m
    identity,
  ),
  Dense(100, 2),                        [90m# 202 parameters[39m
)[90m                   # Total: 18 arrays, [39m33_413 parameters, 130.172 KiB.</code></pre><p>Of course you can also create a custom backbone using the functions present in <code>FastAI.Models</code>.</p><pre lang=julia ><code>cardinalities = collect(map(col -&gt; length(catdict[col]), cat))

ovdict = Dict(:workclass =&gt; 10, :education =&gt; 12, Symbol(&quot;native-country&quot;) =&gt; 16)
overrides = collect(map(col -&gt; col in keys(ovdict) ? ovdict[col] : nothing, cat))

embedszs = FastAI.Models.get_emb_sz(cardinalities, overrides)
catback = FastAI.Models.tabular_embedding_backbone(embedszs, 0.2);</code></pre><p>We can then pass a named tuple <code>(categorical = ..., continuous = ...)</code> to <code>methodmodel</code> to replace the default backbone.</p><pre lang=julia ><code>backbone = (categorical = catback, continuous =  BatchNorm(length(cont)))
model = methodmodel(method, backbone)</code></pre><pre class=coderesult ><code>Chain(
  Parallel(
    vcat,
    Chain(
      FastAI.Models.var"#41#43"(),
      Parallel(
        vcat,
        Embedding(10, 10),              [90m# 100 parameters[39m
        Embedding(17, 12),              [90m# 204 parameters[39m
        Embedding(8, 5),                [90m# 40 parameters[39m
        Embedding(17, 8),               [90m# 136 parameters[39m
        Embedding(7, 5),                [90m# 35 parameters[39m
        Embedding(6, 4),                [90m# 24 parameters[39m
        Embedding(3, 3),                [90m# 9 parameters[39m
        Embedding(43, 16),              [90m# 688 parameters[39m
      ),
      Dropout(0.2),
    ),
    BatchNorm(6),                       [90m# 12 parameters[39m[90m, plus 12[39m
  ),
  Chain(
    Dense(69, 200, relu; bias=false),   [90m# 13_800 parameters[39m
    BatchNorm(200),                     [90m# 400 parameters[39m[90m, plus 400[39m
    identity,
  ),
  Chain(
    Dense(200, 100, relu; bias=false),  [90m# 20_000 parameters[39m
    BatchNorm(100),                     [90m# 200 parameters[39m[90m, plus 200[39m
    identity,
  ),
  Dense(100, 2),                        [90m# 202 parameters[39m
)[90m                   # Total: 18 arrays, [39m35_850 parameters, 138.828 KiB.</code></pre><p>To directly get a <a href=../REFERENCE/FluxTraining.Learner.html ><code>Learner</code></a> suitable for our method and data, we can use the <a href=../REFERENCE/FastAI.methodlearner.html ><code>methodlearner</code></a> function. This creates both batched data loaders and a model for us.</p><pre lang=julia ><code>learner = methodlearner(method, splitdata;
    backbone=backbone, callbacks=[Metrics(accuracy)],
    batchsize=128, buffered=false)</code></pre><pre class=coderesult ><code>Learner()</code></pre><p>Once we have a <code>Learner</code>, we can call <a href=../REFERENCE/FastAI.fitonecycle!.html ><code>fitonecycle!</code></a> on it to train it for the desired number of epochs:</p><pre lang=julia ><code>fitonecycle!(learner, 3, 0.2)</code></pre><pre class=codeoutput ><code><span class="sgr32">Epoch 1 TrainingPhase(): 100%|██████████████████████████| Time: 0:01:00</span>
┌───────────────┬───────┬─────────┬──────────┐
│<span class="sgr1">         Phase </span>│<span class="sgr1"> Epoch </span>│<span class="sgr1">    Loss </span>│<span class="sgr1"> Accuracy </span>│
├───────────────┼───────┼─────────┼──────────┤
│ TrainingPhase │   1.0 │ 0.37405 │  0.82753 │
└───────────────┴───────┴─────────┴──────────┘
<span class="sgr32">Epoch 1 ValidationPhase(): 100%|████████████████████████| Time: 0:00:02</span>
┌─────────────────┬───────┬─────────┬──────────┐
│<span class="sgr1">           Phase </span>│<span class="sgr1"> Epoch </span>│<span class="sgr1">    Loss </span>│<span class="sgr1"> Accuracy </span>│
├─────────────────┼───────┼─────────┼──────────┤
│ ValidationPhase │   1.0 │ 0.39243 │  0.81782 │
└─────────────────┴───────┴─────────┴──────────┘
<span class="sgr32">Epoch 2 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:04</span>
┌───────────────┬───────┬─────────┬──────────┐
│<span class="sgr1">         Phase </span>│<span class="sgr1"> Epoch </span>│<span class="sgr1">    Loss </span>│<span class="sgr1"> Accuracy </span>│
├───────────────┼───────┼─────────┼──────────┤
│ TrainingPhase │   2.0 │ 0.35332 │  0.83909 │
└───────────────┴───────┴─────────┴──────────┘
<span class="sgr32">Epoch 2 ValidationPhase(): 100%|████████████████████████| Time: 0:00:00</span>
┌─────────────────┬───────┬─────────┬──────────┐
│<span class="sgr1">           Phase </span>│<span class="sgr1"> Epoch </span>│<span class="sgr1">    Loss </span>│<span class="sgr1"> Accuracy </span>│
├─────────────────┼───────┼─────────┼──────────┤
│ ValidationPhase │   2.0 │ 0.33674 │  0.84259 │
└─────────────────┴───────┴─────────┴──────────┘
<span class="sgr32">Epoch 3 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:04</span>
┌───────────────┬───────┬─────────┬──────────┐
│<span class="sgr1">         Phase </span>│<span class="sgr1"> Epoch </span>│<span class="sgr1">    Loss </span>│<span class="sgr1"> Accuracy </span>│
├───────────────┼───────┼─────────┼──────────┤
│ TrainingPhase │   3.0 │ 0.32081 │  0.85238 │
└───────────────┴───────┴─────────┴──────────┘
<span class="sgr32">Epoch 3 ValidationPhase(): 100%|████████████████████████| Time: 0:00:00</span>
┌─────────────────┬───────┬─────────┬──────────┐
│<span class="sgr1">           Phase </span>│<span class="sgr1"> Epoch </span>│<span class="sgr1">    Loss </span>│<span class="sgr1"> Accuracy </span>│
├─────────────────┼───────┼─────────┼──────────┤
│ ValidationPhase │   3.0 │ 0.31522 │  0.85259 │
└─────────────────┴───────┴─────────┴──────────┘
</code></pre></article><footer class=book-footer ></footer></div><aside class=book-toc ><nav id=toc class=book-toc-content ><ul><li><a href=#tabular-classification >Tabular Classification</a><ul></ul></li></ul></nav></aside></main></body></HTML>